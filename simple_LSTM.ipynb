{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arnaudstiegler/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import numpy as np\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Bidirectional\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (1175509, 3)\n",
      "Val shape :  (130613, 3)\n",
      "Test shape :  (56370, 2)\n",
      "0.9381297803759903\n",
      "0.06187021962400968\n",
      "0.938130201434773\n",
      "0.06186979856522704\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"./input/train.csv\")\n",
    "train_df, val_df = train_test_split(train, test_size=0.1,stratify=train['target'])\n",
    "test = pd.read_csv(\"./input/test.csv\")\n",
    "print(\"Train shape : \",train_df.shape)\n",
    "print(\"Val shape : \",val_df.shape)\n",
    "print(\"Test shape : \",test.shape)\n",
    "print(train_df.loc[train_df['target']==0].shape[0]/train_df.shape[0])\n",
    "print(train_df.loc[train_df['target']==1].shape[0]/train_df.shape[0])\n",
    "print(val_df.loc[val_df['target']==0].shape[0]/val_df.shape[0])\n",
    "print(val_df.loc[val_df['target']==1].shape[0]/val_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(sentences, verbose =  True):\n",
    "    \"\"\"\n",
    "    :param sentences: list of list of words\n",
    "    :return: dictionary of words and their count\n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "    for sentence in tqdm(sentences, disable = (not verbose)):\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1175509/1175509 [00:08<00:00, 140842.27it/s]\n",
      "100%|██████████| 1175509/1175509 [00:05<00:00, 203108.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Which': 42531, 'former': 627, 'presidents': 162, 'have': 75132, 'maintained': 95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = train_df[\"question_text\"].progress_apply(lambda x: x.split()).values\n",
    "vocab = build_vocab(sentences)\n",
    "print({k: vocab[k] for k in list(vocab)[:5]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arnaudstiegler/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/arnaudstiegler/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train_df[\"question_text\"] = train_df[\"question_text\"].copy().apply(lemmatize_text)\n",
    "val_df[\"question_text\"] = val_df[\"question_text\"].copy().apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196017it [03:48, 9590.74it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2196016 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('./input/embedding/glove.840B.300d.txt')\n",
    "for line in tqdm(f):\n",
    "    values = line.split(\" \")\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130613/130613 [00:40<00:00, 3191.59it/s]\n"
     ]
    }
   ],
   "source": [
    "# Convert values to embeddings\n",
    "def text_to_array(text):\n",
    "    empty_emb = np.zeros(300)\n",
    "    text = text[:-1][:30]\n",
    "    embeds = [embeddings_index.get(x, empty_emb) for x in text]\n",
    "    embeds+= [empty_emb] * (30 - len(embeds))\n",
    "    return np.array(embeds)\n",
    "\n",
    "# train_vects = [text_to_array(X_text) for X_text in tqdm(train_df[\"question_text\"])]\n",
    "val_vects = np.array([text_to_array(X_text) for X_text in tqdm(val_df[\"question_text\"])])\n",
    "val_y = np.array(val_df[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data providers\n",
    "batch_size = 128\n",
    "\n",
    "def batch_gen(train_df):\n",
    "    n_batches = math.ceil(len(train_df) / batch_size)\n",
    "    while True: \n",
    "        train_df = train_df.sample(frac=1.)  # Shuffle the data.\n",
    "        for i in range(n_batches):\n",
    "            texts = train_df.iloc[i*batch_size:(i+1)*batch_size, 1]\n",
    "            text_arr = np.array([text_to_array(text) for text in texts])\n",
    "            yield text_arr, np.array(train_df[\"target\"][i*batch_size:(i+1)*batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True),\n",
    "                        input_shape=(30, 300)))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "100/100 [==============================] - 229s - loss: 0.1923 - acc: 0.9376 - val_loss: 0.3290 - val_acc: 0.9380\n",
      "Epoch 2/5\n",
      "100/100 [==============================] - 236s - loss: 0.1908 - acc: 0.9401 - val_loss: 0.2502 - val_acc: 0.9381\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 234s - loss: 0.1914 - acc: 0.9387 - val_loss: 0.2388 - val_acc: 0.9381\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 259s - loss: 0.1907 - acc: 0.9373 - val_loss: 0.2507 - val_acc: 0.9381\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 284s - loss: 0.1981 - acc: 0.9345 - val_loss: 0.2713 - val_acc: 0.9380\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f16d392b0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mg = batch_gen(train)\n",
    "model.fit_generator(mg, epochs=5,\n",
    "                    steps_per_epoch=100,\n",
    "                    validation_data=(val_vects, val_y),\n",
    "                    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "221it [01:46,  2.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# prediction part\n",
    "batch_size = 256\n",
    "def batch_gen(test_df):\n",
    "    n_batches = math.ceil(len(test_df) / batch_size)\n",
    "    for i in range(n_batches):\n",
    "        texts = test_df.iloc[i*batch_size:(i+1)*batch_size, 1]\n",
    "        text_arr = np.array([text_to_array(text) for text in texts])\n",
    "        yield text_arr\n",
    "\n",
    "test_df = pd.read_csv(\"./input/test.csv\")\n",
    "\n",
    "all_preds = []\n",
    "for x in tqdm(batch_gen(test_df)):\n",
    "    all_preds.extend(model.predict(x).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_te = (np.array(all_preds) > 0.5).astype(np.int)\n",
    "\n",
    "submit_df = pd.DataFrame({\"qid\": test_df[\"qid\"], \"prediction\": y_te})\n",
    "submit_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
