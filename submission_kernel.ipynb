{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\n\nimport pandas as pd\nimport numpy as np\nimport re\n\n\n## some config values \nembed_size = 300 # how big is each word vector\nmax_features = 120000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 72 # max number of words in a question to use\n\n\ntrain = pd.read_csv(\"../input/train.csv\")\n# TO REMOVE\n#train = train.head(10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dd27e72394c10595b1f41bc4e0aa5b0473d4b313"
      },
      "cell_type": "code",
      "source": "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n\ndef clean_contractions(text, mapping):\n    specials = [\"’\", \"‘\", \"´\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n    return text\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "874a0bea28fb53c498133da37d3209381080c6a6"
      },
      "cell_type": "code",
      "source": "punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', }\npunct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n\ndef clean_special_chars(text, punct, mapping):\n    for p in mapping:\n        text = text.replace(p, mapping[p])\n    \n    for p in punct:\n        text = text.replace(p, f' {p} ')\n    \n    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}  # Other special characters that I have to deal with in last\n    for s in specials:\n        text = text.replace(s, specials[s])\n    \n    return text\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "184ed3ab75d165511770cda3f2ed5703c78a34d9"
      },
      "cell_type": "code",
      "source": "def clean_numbers(x):\n    x = re.sub('[0-9]{5,}', '#####', x)\n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}', '##', x)\n    return x",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "train[\"question_text\"] = train[\"question_text\"].str.lower()\ntrain['question_text'] = train['question_text'].apply(lambda x: clean_special_chars(x, punct, punct_mapping))\ntrain['question_text'] = train['question_text'].apply(lambda x: clean_numbers(x))\ntrain['question_text'] = train['question_text'].apply(lambda x: clean_contractions(x, contraction_mapping))\ntexts = train[\"question_text\"].fillna(\"_##_\").values\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(texts)\nsequences = tokenizer.texts_to_sequences(texts)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\ndata = pad_sequences(sequences, maxlen=maxlen)\n\nlabels = train['target'].values\n\n#Shuffling the data\nnp.random.seed(438)\npermut = np.random.permutation(len(data))\ndata = data[permut]\nlabels = labels[permut]\nx_train, x_val, y_train, y_val = train_test_split(data, labels, test_size=0.3, random_state=42)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "807cc1c0b712a17827dc3f00d15c88d8f3182c6f"
      },
      "cell_type": "code",
      "source": "def glove_matrix(word_index):\n    embeddings_index = {}\n    f = open('../input/embeddings/glove.840B.300d/glove.840B.300d.txt')\n    for line in f:\n        values = line.split(\" \")\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n    f.close()\n    \n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n    embed_size = all_embs.shape[1]\n    print('Found %s word vectors.' % len(embeddings_index))\n    \n    nb_words = min(max_features, len(word_index)+1)\n    \n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    for word, i in word_index.items():\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            if i >= max_features: continue\n            embedding_matrix[i] = embedding_vector\n    return embedding_matrix\n\nmatrix_glove = glove_matrix(word_index)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3f9d2c717de94385f9f7cea3a5a91ca58108228f"
      },
      "cell_type": "code",
      "source": "np.shape(matrix_glove)\n#np.shape(matrix_para)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a9a3a8bf8dc23758c21c7cea67957e3e29028ee3"
      },
      "cell_type": "code",
      "source": "from tqdm import tqdm\n\ndef load_para(word_index):\n    EMBEDDING_FILE = '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = -0.0053247833,0.49346462\n    embed_size = all_embs.shape[1]\n\n    nb_words = min(max_features, len(word_index)+1)\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    for word, i in tqdm(word_index.items()):\n        if i >= max_features: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n    \n    return embedding_matrix\nmatrix_para = load_para(word_index)\n\n# Averaging the two embedding matrix\nembedding_matrix = np.mean([matrix_glove, matrix_para], axis = 0)\nnp.shape(embedding_matrix)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "53fc2f292f54fd797166ca391fddefa2ad87f558"
      },
      "cell_type": "code",
      "source": "from keras import backend as K\n\ndef f1(y_true, y_pred):\n    def recall(y_true, y_pred):\n        \"\"\"Recall metric.\n\n        Only computes a batch-wise average of recall.\n\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives / (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        \"\"\"Precision metric.\n\n        Only computes a batch-wise average of precision.\n\n        Computes the precision, a metric for multi-label classification of\n        how many selected items are relevant.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives / (predicted_positives + K.epsilon())\n        return precision\n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "476eba3ac37760a00d00179b28229ec12bd4d75d"
      },
      "cell_type": "code",
      "source": "# https://www.kaggle.com/suicaokhoailang/lstm-attention-baseline-0-652-lb\nfrom keras.layers import Layer\nfrom keras import initializers, regularizers, constraints\n\nclass Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8d6a819d3166f6144f2ce29d143c0316b74a9e85"
      },
      "cell_type": "code",
      "source": "from keras.layers import Embedding,Input,SpatialDropout1D,Bidirectional,CuDNNLSTM,CuDNNGRU,Dense,Flatten,Dropout,BatchNormalization\nfrom keras.initializers import glorot_normal\nfrom keras.models import Model\n\nembedding_layer = Embedding(max_features,\n                            embed_size,\n                            weights=[embedding_matrix],\n                            input_length=maxlen,\n                            trainable=False)\nsequence_input = Input(shape=(maxlen,), dtype='int32')\nembedded_sequences = embedding_layer(sequence_input)\nx = SpatialDropout1D(0.1)(embedded_sequences)\nx = Bidirectional(CuDNNLSTM(100, return_sequences=True, \n                                kernel_initializer=glorot_normal(seed=12300), recurrent_initializer='orthogonal'))(x)\nx = Flatten()(x)\nx = Dense(100, activation=\"relu\", kernel_initializer=glorot_normal(seed=12300))(x)\nx = Dropout(0.1)(x)\nx = BatchNormalization()(x)\noutp = Dense(1, activation=\"sigmoid\")(x)\n\nmodel_gru = Model(sequence_input, outp )\nmodel_gru.compile(loss='binary_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bdd4d5a8ea746edf84dae195b013cd329cc42c9a"
      },
      "cell_type": "code",
      "source": "outputs = []\n\nmodel_gru_score = model_gru.fit(x_train, y_train, validation_data=(x_val, y_val),epochs=4, batch_size=128,verbose=2)\nmodel_gru_score = model_gru_score.history\nmodel_gru_score = np.array(model_gru_score['val_loss'])\nmodel_gru_score = np.mean(model_gru_score)\nprint(model_gru_score)\ny_val_pred_gru = model_gru.predict(x_val)\n\noutputs.append([y_val_pred_gru, model_gru_score, 'LSTM #1'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b97a74f473f6be0095e8e88e9121e27341ec83f2"
      },
      "cell_type": "code",
      "source": "from keras.layers import GlobalAveragePooling1D,GlobalMaxPooling1D,concatenate\n\ninp = Input(shape=(maxlen,))\nx = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\nx = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\navg_pool = GlobalAveragePooling1D()(x)\nmax_pool = GlobalMaxPooling1D()(x)\nconc = concatenate([avg_pool, max_pool])\nconc = Dense(64, activation=\"relu\")(conc)\nconc = Dropout(0.1)(conc)\noutp = Dense(1, activation=\"sigmoid\")(conc)\n    \nmodel_gru_2 = Model(inputs=inp, outputs=outp)\nmodel_gru_2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0b7d1878052e3b60f2afe62ddf9e5a96b6fdacda",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "model_gru_2_score = model_gru_2.fit(x_train, y_train, validation_data=(x_val, y_val),epochs=3, batch_size=128,verbose=2)\nmodel_gru_2_score = model_gru_2_score.history\nmodel_gru_2_score = np.array(model_gru_2_score['val_loss'])\nmodel_gru_2_score = np.mean(model_gru_2_score)\nprint(model_gru_2_score)\ny_val_pred_gru_2 = model_gru_2.predict(x_val)\n\noutputs.append([y_val_pred_gru_2, model_gru_2_score, 'LSTM #2'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d6bc4c8dc07394e94b2297de9a38e78ecee0b9f6"
      },
      "cell_type": "code",
      "source": "from keras.layers import Reshape,Conv2D,MaxPool2D,Concatenate\n\nfilter_sizes = [1,2,3,5]\nnum_filters = 36\n\ninp = Input(shape=(maxlen,))\nx = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\nx = Reshape((maxlen, embed_size, 1))(x)\n\nmaxpool_pool = []\nfor i in range(len(filter_sizes)):\n    conv = Conv2D(num_filters, kernel_size=(filter_sizes[i], embed_size),\n                                     kernel_initializer='he_normal', activation='elu')(x)\n    maxpool_pool.append(MaxPool2D(pool_size=(maxlen - filter_sizes[i] + 1, 1))(conv))\n\nz = Concatenate(axis=1)(maxpool_pool)   \nz = Flatten()(z)\nz = Dropout(0.1)(z)\noutp = Dense(1, activation=\"sigmoid\")(z)\n\nmodel_cnn = Model(inputs=inp, outputs=outp)\nmodel_cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "28c7e8d5693ed7d5a1c31135ab7cbbb576e2d80d"
      },
      "cell_type": "code",
      "source": "model_cnn_score = model_cnn.fit(x_train, y_train, validation_data=(x_val, y_val),epochs=4, batch_size=128,verbose=2)\nmodel_cnn_score = model_cnn_score.history\nmodel_cnn_score = np.array(model_cnn_score['val_loss'])\nmodel_cnn_score = np.mean(model_cnn_score)\nprint(model_cnn_score)\ny_val_pred_cnn = model_cnn.predict(x_val)\n\noutputs.append([y_val_pred_cnn, model_cnn_score, 'CNN'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "23a960d0b58de41746870eced3d8ee06a6ca0a1c"
      },
      "cell_type": "code",
      "source": "inp = Input(shape=(maxlen,))\nx = Embedding(max_features,\n                            embed_size,\n                            weights=[embedding_matrix],\n                            input_length=maxlen,\n                            trainable=False)(inp)\nx = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\nx = Attention(maxlen)(x) # New\nx = Dense(16, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\nx = Dense(1, activation=\"sigmoid\")(x)\nmodel_gru_att = Model(inputs=inp, outputs=x)\nmodel_gru_att.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "eda27d53eac7e9ecad5ef4ffd7a3081f376a338b"
      },
      "cell_type": "code",
      "source": "model_gru_att_score = model_gru_att.fit(x_train, y_train, validation_data=(x_val, y_val),epochs=4, batch_size=128,verbose=2)\nmodel_gru_att_score = model_gru_att_score.history\nmodel_gru_att_score = np.array(model_gru_att_score['val_loss'])\nmodel_gru_att_score = np.mean(model_gru_att_score)\nprint(model_gru_att_score)\ny_val_pred_gru_att = model_gru_att.predict(x_val)\n\noutputs.append([y_val_pred_gru_att, model_gru_att_score , 'CNN'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "79e7d08befdf2fbe3423600d6b2463b4ff07d3f3"
      },
      "cell_type": "code",
      "source": "outputs.sort(key=lambda x: x[1]) # Sort the output by val f1 score\nweights = [i for i in range(1, len(outputs) + 1)]\nweights = [float(i) / sum(weights) for i in weights] \nprint(weights)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": false,
        "_kg_hide-output": false,
        "trusted": true,
        "_uuid": "b633184143a3d247a657cc0d08e04b5391620b57"
      },
      "cell_type": "code",
      "source": "from sklearn.linear_model import LinearRegression\nX = np.asarray([outputs[i][0] for i in range(len(outputs))])\nX = X[...,0]\nreg = LinearRegression().fit(X.T, y_val)\nprint(reg.score(X.T, y_val),reg.coef_)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f386912f856eb1a9d141ae0bf0c402dedb37814c"
      },
      "cell_type": "code",
      "source": "from sklearn.metrics import roc_curve, precision_recall_curve\n\ndef threshold_search(y_true, y_proba, plot=False):\n    precision, recall, thresholds = precision_recall_curve(y_true, y_proba)\n    thresholds = np.append(thresholds, 1.001) \n    F = 2 / (1/precision + 1/recall)\n    best_score = np.max(F)\n    best_th = thresholds[np.argmax(F)]\n    if plot:\n        plt.plot(thresholds, F, '-b')\n        plt.plot([best_th], [best_score], '*r')\n        plt.show()\n    search_result = {'threshold': best_th , 'f1': best_score}\n    return search_result\n\n#Averaging\ny_val_pred = np.array([y_val_pred_gru, y_val_pred_cnn,y_val_pred_gru_2,y_val_pred_gru_att])\ny_val_pred = np.average(y_val_pred, axis=0)\n\n#Blending with LR\n#y_val_pred = np.sum([outputs[i][0] * reg.coef_[i] for i in range(len(outputs))], axis = 0)\n\nthresh_results = threshold_search(y_val,y_val_pred)\nthreshold = thresh_results['threshold']\nprint(thresh_results['threshold'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "eb9afb1afb7be1f0d43f474a8a73a6ffd56dfb1b"
      },
      "cell_type": "code",
      "source": "test = pd.read_csv(\"../input/test.csv\")\n# TO REMOVE \n#test = test.head(10)\ntest[\"question_text\"] = test[\"question_text\"].str.lower()\ntest['question_text'] = test['question_text'].apply(lambda x: clean_special_chars(x, punct, punct_mapping))\ntest['question_text'] = test['question_text'].apply(lambda x: clean_numbers(x))\ntest['question_text'] = test['question_text'].apply(lambda x: clean_contractions(x, contraction_mapping))\ntest_texts = test[\"question_text\"].fillna(\"_##_\").values\n\ntest_sequences = tokenizer.texts_to_sequences(test_texts)\ntest_data = pad_sequences(test_sequences, maxlen=maxlen)\n\ny_pred_gru = model_gru.predict(test_data)\ny_pred_gru_2 = model_gru.predict(test_data)\ny_pred_cnn = model_cnn.predict(test_data)\ny_pred_gru_att = model_gru_att.predict(test_data)\n\ntest_outputs = [y_pred_gru,y_pred_gru_2,y_pred_cnn,y_pred_gru_att]\n\n#Averaging predictions\ny_pred = np.array([y_pred_gru, y_pred_cnn,y_pred_gru_2,y_pred_gru_att])\ny_pred = np.average(y_pred, axis=0)\n\n#Blending with LR\n#y_pred = np.sum([test_outputs[i] * reg.coef_[i] for i in range(len(outputs))], axis = 0)\n\ny_threshed = (np.array(y_pred) > threshold).astype(np.int)\nflat_list = [item for sublist in y_threshed for item in sublist]\n\nsubmit_df = pd.DataFrame({\"qid\": test[\"qid\"].values, \"prediction\": flat_list})\nsubmit_df.to_csv(\"submission.csv\", index=False)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}